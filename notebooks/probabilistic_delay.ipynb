{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a26952-3024-400b-a9b8-d5b70e488f60",
   "metadata": {},
   "source": [
    "# Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95340a67-d8de-46e4-be6c-7adeaedf2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d26cfe-7607-4c63-9159-71d3ec60e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython import get_ipython\n",
    "server = \"http://iccluster044.iccluster.epfl.ch:8998\"\n",
    "username = os.environ['RENKU_USERNAME']\n",
    "\n",
    "get_ipython().run_cell_magic(\n",
    "    'spark',\n",
    "    line='config', \n",
    "    cell=\"\"\"{{ \"name\": \"{0}-final\", \"executorMemory\": \"4G\", \"executorCores\": 4, \"numExecutors\": 10, \"driverMemory\": \"4G\" }}\"\"\".format(username)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017199f2-c94f-4986-8667-fc12dc7978c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic(\n",
    "    \"spark\", f\"\"\"add -s {username}-final -l python -u {server} -k\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09c029-d523-414c-aa09-34b5032c51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c4e68-81d9-43ef-8bb7-d3a7b12439bd",
   "metadata": {},
   "source": [
    "# Estimation of the parameters from the data :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f47bc-7ca7-4e72-9449-449a37b97af5",
   "metadata": {},
   "source": [
    "#### Import :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327f1b0-dbdb-41f0-822e-ee7afe2ec0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#import the isdaten df to retrieve the real arrival time\n",
    "isdt = spark.read.orc('/data/sbb/part_orc/istdaten/')\n",
    "\n",
    "#import the nodes zurich df to have the node list of interest\n",
    "nodes = spark.read.orc('/group/grande_envergure/graph/nodes_zurich.orc').select('stop_id').rdd.map(lambda row: row[0].split(':')[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5ec53-925f-4b17-8e62-eb11f252fa32",
   "metadata": {},
   "source": [
    "#### First data processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3acef05-2177-4953-8d5b-3063f333c0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.sql.functions import to_timestamp, col, dayofweek\n",
    "\n",
    "#clean the data of isdt to keep only the interesting nodes\n",
    "isdt_clean = isdt.where((col('bpuic').isin(nodes)\n",
    "                        & (col('zusatzfahrt_tf') == 'false')\n",
    "                        & (col('faellt_aus_tf') == 'false')\n",
    "                        & (col('an_prognose_status') != 'UNBEKANNT')\n",
    "                        & (col('ab_prognose') != ''))\n",
    "                        & (col('durchfahrt_tf') == 'false')\n",
    "                        & (dayofweek(to_timestamp(col('an_prognose'), \"dd.MM.yyyy HH:mm:ss\")) % 7 > 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bd11b-69b9-421b-b9ad-816a9e926a85",
   "metadata": {},
   "source": [
    "We can now compute the delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5eee1d-d998-4f8c-ae43-c83fa8d29f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.sql.functions import expr, hour\n",
    "\n",
    "#compute the delay at each nodes and put it to zero if it the transport arrive earlier\n",
    "isdt_delay = isdt_clean.withColumn(\"delay\", to_timestamp(col('an_prognose'), \"dd.MM.yyyy HH:mm:ss\").cast('long')-to_timestamp(col('ankunftszeit'), \"dd.MM.yyyy HH:mm\").cast('long'))\n",
    "isdt_delay = isdt_delay.withColumn(\"delay\", expr(\"CASE WHEN delay < 0 THEN 0 ELSE delay END\"))\n",
    "\n",
    "#store the hour of the transport arrival at the stop for further preocessing\n",
    "isdt_delay = isdt_delay.withColumn(\"hour\", hour(to_timestamp(col('ankunftszeit'),\"dd.MM.yyyy HH:mm\"))).cache()\n",
    "\n",
    "#store in hdfs the delay table for validation\n",
    "isdt_delay_100days = isdt_delay.select('betriebstag').distinct().sample(0.05) # sample 5% of isdt_delay for validation\n",
    "isdt_delay.join(isdt_delay_100days, how='inner', on='betriebstag').select(col('bpuic').alias('stop_id'),\n",
    "                                                                          col('hour'),\n",
    "                                                                          col('delay')).write.save(\"/group/grande_envergure/graph/isdt_delay.orc\",format=\"orc\", mode='overwrite'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d3346-9151-4f47-be1f-c12a9794e15b",
   "metadata": {},
   "source": [
    "#### Data visualization\n",
    "\n",
    "We used this code to look how are distributed the delays at a certain random stop at a certain random hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cf85c-4b94-4887-8443-4f15b2ff6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o delay_df\n",
    "\n",
    "delay_df = isdt_delay.where((col('bpuic') == '8591304') & (col('hour') == '20')).select('delay').alias('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181c5af-d1ba-45ae-9b34-39c2b6a32273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#define specific parameters for the plot\n",
    "plt.rcParams['figure.figsize'] = (10,8)\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot histogram using Pandas\n",
    "plt.hist(delay_df['delay'], bins='auto')\n",
    "plt.xlabel('Delay')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Delays for stop : 8591304 at 20')\n",
    "\n",
    "# Show the plot using display()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9229005d-70a6-448a-8727-d0b67b39dcd2",
   "metadata": {},
   "source": [
    "We can see that it would be reasonable to assume exponential distribution for the delays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e334d-d1c4-4873-957e-6bc1745a374b",
   "metadata": {},
   "source": [
    "#### Computing mean delays :\n",
    "\n",
    "To compute the average delays at the different stops we decided to group the data by the stop_id (bpuic) and the hour of the day because we expect similar delay model for a same stop at a certain hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e527b-3b75-401d-b5bf-25aec9ce3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "isdt_delay_mean = isdt_delay.groupBy(['bpuic','hour']).agg(expr(\"AVG(delay)\").alias('avg_delay')).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5882235-ef7c-4ba3-916a-3ccc9d57e8ce",
   "metadata": {},
   "source": [
    "#### Saving the table :\n",
    "\n",
    "We now save the table to hdfs to use it outside of spark later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a176b4-fb98-45e3-8bd5-bbed4454d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "#we select only the columns we'll use later to estimate the probability of catching a correspondance at a certain stop\n",
    "isdt_delay_mean.select(col('bpuic').alias('stop_id'),\n",
    "                       col('hour'),\n",
    "                       col('avg_delay')).write.save(\"/group/grande_envergure/graph/isdt_delay_mean.orc\",format=\"orc\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be921a-fd4b-486e-809d-b030aa7751f8",
   "metadata": {},
   "source": [
    "## Proba computing :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb23904-83ca-4277-a6de-0d7e51b83cbb",
   "metadata": {},
   "source": [
    "#### Import :\n",
    "\n",
    "We first need to creat a SparkSession to import the dataframe we created before and transform it into a pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965213e6-c98c-4afe-8a1b-ea60e23ebc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "#create the SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Router\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeeb492-fb39-4469-abac-da8cfdfc8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the table from hdfs\n",
    "isdt = spark.read.orc('/group/grande_envergure/graph/isdt_delay_mean.orc')\n",
    "\n",
    "#transform it to a pandas df\n",
    "df_mean = isdt.toPandas()\n",
    "df_mean.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b7c0c7-1985-473e-b107-4088deba5598",
   "metadata": {},
   "source": [
    "#### Proba estimation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67a75e4-031d-4d1c-a07f-52d982a1cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_sec(hour):\n",
    "    '''\n",
    "    Transform a time given in a string format to second\n",
    "    input:\n",
    "        - string(hour): The string datetime in the format 'hh:mm:ss'\n",
    "    output:\n",
    "        - float(second): The number of second corresponding to the hour string\n",
    "    '''\n",
    "    split = hour.split(':')\n",
    "    return float(split[0])*60*60 + float(split[1])*60 + float(split[2])\n",
    "\n",
    "def exp_prob(m_delay,f_time):\n",
    "    '''\n",
    "    Compute the probability of having a delay greater or equal than f_time\n",
    "    following a exponential distribution\n",
    "    input:\n",
    "        -float(m_delay) : mean delay of the exponential distribution\n",
    "        -float(f_time) : maximum free time we have before the next connection\n",
    "    output:\n",
    "        -float(proba) : the probability of having a delay greater or equal to f_time\n",
    "    '''\n",
    "    if f_time == 0 :\n",
    "        return 0\n",
    "    #if the m_delay is zero the probability is always equal to 1\n",
    "    if m_delay == 0:\n",
    "        return 1.0\n",
    "    return 1.0 - np.exp(-f_time / m_delay)\n",
    "\n",
    "def trip_conf(path, stop_means):\n",
    "    '''\n",
    "    Compute the probability of catching every connection of a given path\n",
    "    input:\n",
    "        -list(dict)(path): a list of edges that compose a path\n",
    "        -df(stop_means): the dataframe storing all the avg_delays\n",
    "    output:\n",
    "        -float(proba): porbability of catching every connection of the path\n",
    "    '''\n",
    "    c_trip = '' #stores the current trip id\n",
    "    f_time = 0  #stores the free time we have to catch a connection         \n",
    "    prev_stop_time = 0 #stores the last arrival time for the current trip\n",
    "    m_delay = 0 #stores the mean delay of the last stop we took on a trip\n",
    "    p = 1.0 #stores probability of making all the connections\n",
    "    avg_delay = stop_means['avg_delay'].mean() #stores the total average delay\n",
    "    \n",
    "    #we'll go through all the edges in the path\n",
    "    for edge in path:\n",
    "        \n",
    "        #if the edge is a walking edge\n",
    "        if edge['trip_id'] == 'None':\n",
    "            c_trip = 'None'\n",
    "            #we compute the walking time and substract it to the actual free time available (margin of 10 sec)\n",
    "            f_time-=to_sec(edge['end_time'])-to_sec(edge['start_time'])-10\n",
    "            continue\n",
    "        \n",
    "        #if we're changing trip (that means it is a connection)\n",
    "        elif edge['trip_id'] != c_trip:\n",
    "            c_trip = edge['trip_id']\n",
    "            \n",
    "            #if it is not the first edge (not counting the walking edges)\n",
    "            if prev_stop_time != 0:\n",
    "                #we compute the free time we have between the previous arrival and the next departure and add it to the free time\n",
    "                f_time+=to_sec(edge['start_time'])-prev_stop_time\n",
    "                #compute the probability to make the connection and multiply it to the total probability\n",
    "                #(we assume that the delay experienced at different connection are independant)\n",
    "                p*= exp_prob(m_delay,f_time)\n",
    "                #reset the free time\n",
    "                f_time = 0\n",
    "        \n",
    "        #actualize information about the stop\n",
    "        prev_stop_time = to_sec(edge['end_time'])\n",
    "        \n",
    "        #we check if we have an avg delay for this stop at this hour of the day in our dataframe, if not we set the avg delay to the\n",
    "        #total avg delay for all the stop\n",
    "        if stop_means[(stop_means['stop_id'] == edge['end_stop_id']) & (stop_means['hour'] == prev_stop_time//3600)].any(axis=0).any():\n",
    "            m_delay = stop_means.loc[(stop_means['stop_id'] == edge['end_stop_id']) & (stop_means['hour'] == prev_stop_time//3600), 'avg_delay'].values[0]\n",
    "        else:\n",
    "            m_delay = avg_delay\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f81c14e-784b-4ab5-b3b9-99258e88bb41",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f291c83-1a9b-419b-9e16-0d3549e1ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "isdt_delay = spark.read.orc('/group/grande_envergure/graph/isdt_delay.orc')\n",
    "isdt_delay.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db8225-941c-4089-ae40-10eaee7869b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "isdt_delay = isdt_delay.repartition('stop_id', 'hour').cache()\n",
    "\n",
    "def val_prob(stop_delay, isdt_delay, f_time, num_all_stops_hours):\n",
    "    '''\n",
    "    Compute the probability of having a delay greater or equal than f_time\n",
    "    following a exponential distribution\n",
    "    input:\n",
    "        -spark_df(stop_delay) : spark dataframe of delays for a certain stop & hour\n",
    "        -float(f_time) : maximum free time we have before the next connection\n",
    "    output:\n",
    "        -float(proba) : the probability of having a delay smaller or equal to f_time\n",
    "    '''\n",
    "\n",
    "    num_valid=stop_delay.where(col('delay') <= f_time).count()\n",
    "    num_all=stop_delay.count()\n",
    "\n",
    "    # if there are no delays for this stop at this hour suppress condition on stop and hour\n",
    "    # to only check whether the free time is smaller than the delays overall\n",
    "    if num_all == 0:\n",
    "        num_valid=isdt_delay.where(col('delay') <= f_time).count()\n",
    "        num_all=num_all_stops_hours\n",
    "\n",
    "    return num_valid/num_all\n",
    "\n",
    "def trip_val(path, isdt_delay):\n",
    "    '''\n",
    "    Compute the probability of catching every connection of a given path\n",
    "    input:\n",
    "        -list(dict)(path): a list of edges that compose a path\n",
    "        -df(df_delay): the dataframe storing all the avg_delays\n",
    "    output:\n",
    "        -float(proba): probability of catching every connection of the path\n",
    "    '''\n",
    "    c_trip = '' #stores the current trip id\n",
    "    f_time = 0  #stores the free time we have to catch a connection\n",
    "    prev_stop_time = 0 #stores the last arrival time for the current trip\n",
    "    p = 1.0 #stores probability of making all the connections\n",
    "    num_all_stops_hours = isdt_delay.count()\n",
    "\n",
    "    #we'll go through all the edges in the path\n",
    "    for edge in path:\n",
    "\n",
    "        #if the edge is a walking edge\n",
    "        if edge['trip_id'] == 'None':\n",
    "            c_trip = 'None'\n",
    "            #we compute the walking time and substract it to the actual free time available (margin of 10 sec)\n",
    "            f_time-=to_sec(edge['end_time'])-to_sec(edge['start_time'])-10\n",
    "            continue\n",
    "\n",
    "        #if we're changing trip (that means it is a connection)\n",
    "        elif edge['trip_id'] != c_trip:\n",
    "            c_trip = edge['trip_id']\n",
    "\n",
    "            #if it is not the first edge (not counting the walking edges)\n",
    "            if prev_stop_time != 0:\n",
    "                #we compute the free time we have between the previous arrival and the next departure and add it to the free time\n",
    "                f_time+=to_sec(edge['start_time'])-prev_stop_time\n",
    "                #compute the probability to make the connection and multiply it to the total probability\n",
    "                #(we assume that the delay experienced at different connection are independant)\n",
    "                p *= val_prob(stop_delay, isdt_delay, f_time, num_all_stops_hours)\n",
    "                #reset the free time\n",
    "                f_time = 0\n",
    "\n",
    "        #actualize information about the stop\n",
    "        prev_stop_time = to_sec(edge['end_time'])\n",
    "\n",
    "        stop_delay = isdt_delay.where((col('stop_id') == edge['end_stop_id']) & (col('hour') == prev_stop_time//3600))\n",
    "\n",
    "    return p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
